# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XdXoj3Ecd3UQc7RJ0cQkut-BXS9xSsdM

# **CIS 5450 Final Project: Medical Appointment No Shows**

*Taha Iqbal, Suha Memon, Ishwari Mulay*

# Part 1: Introduction

For our project, our group chose to explore data about medical appointment no-shows. We intend to study how various factors influence a patient’s likelihood of missing a scheduled medical appointment (resulting in a no-show). Our objective is to build a predictive model that identifies patients at higher risk of not attending their scheduled appointments, with the hopes of enabling healthcare providers to take additional proactive steps to encourage their patients to actually show up for their appointments. This would enable more efficient resource allocation, reducing the financial losses, and improve appointment turnout. We find this project compelling because it would allow us to understand the effects of demographics, location (and living conditions), health conditions, and patient engagement on no-show appointments, ultimately helping providers identify when additional interventions could improve attendance and enhance patient outcomes in the healthcare system.

Our dataset consists of 110,527 medical appointments and 14 characteristics from Vitória Brazil. We are trying to predict whether a patient shows up to their appointment or does not based on the 13 other variables.

For data exploration, it's essential to check the types of the variables, summarize the statistics, and understand the distribution of key variables. Below are the steps you can take:

The variables in our dataset are as follow:

*    No-show (Target Variable): Indicates whether a patient showed up for their scheduled appointment.
*   Age: The age of the patient at the time of the appointment.

*   Gender: The gender of the patient.
*   ScheduledDay: The date on which the appointment was scheduled.

*   AppointmentDay: The actual date of the appointment.

*   Hypertension: Whether the patient has hypertension.

*   Diabetes: Whether the patient has diabetes.
*   Alcoholism: Whether the patient has a history of alcoholism.

*   Handicap: Whether the patient has a physical handicap or disability.
*   SMS_received: Indicates whether the patient received an SMS reminder about the appointment.


*   Government Aid Recipient: Whether the patient is enrolled in a government or healthcare program that provides financial assistance for medical treatment.
*   Neighborhood: The neighborhood in which the patient lives.

We have the following data types:

*   Categorical variable : Gender, SMS_received, Hypertension, Diabetes, Alcoholism, Handicap, Zip Code, and No-show.
*   Continuous variables : Age, Days_in_between (engineered), and AppointmentDay.
*   Date/Time Variables: ScheduledDay and AppointmentDay (could be processed to extract useful features like the day of the week or time of day).

# Part 2: Data Loading & Preprocessing

For this step of our project, we will be importing the necessary libraries in order to conduct all stages of our report, including pre-processing, exploratory data analysis, data cleaning, and model selection.


We have performed some feature engineering, which may introduce important features and potentially be crucial for improving model performance:


*   Days_in_between: This feature can provide insight into how far in advance appointments are made and its potential relationship with no-shows. It can capture the urgency or planning behavior of patients.
*   Zip Code: Geographical data can be important for modeling appointment attendance. It may be necessary to encode the zip code in a way that captures meaningful patterns (e.g., grouping zip codes into broader regions if necessary).
*   Latitude/Longitude: Geographical coordinates used to plot Neighborhoods onto ineractive map of Vitória, Brazil

## 2.1: Importing the neccessary libraries

We will start by importing the relevant libraries for our report (for all parts of the report).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, f1_score
from xgboost import XGBClassifier
from scipy.stats import randint

"""## 2.2 Loading Data into Pandas Dataframe

We will begin by reading our dataset, which we have uploaded to a GitHub repository that we created for the purpose of this project. We decided to do this so that we would not need to create a new working directory within our project directory.
"""

# Loading Data from GitHub repository
github_url = 'https://raw.githubusercontent.com/suha-memon/CIS5500FinalProject/main/HealthcareNoShowDataset.csv'
# Saving data into Pandas Dataframe called data
raw_data = pd.read_csv(github_url)
data = raw_data.copy()

"""## 2.3 Analyzing Data Structure

In order to better understand the original data, we decided to calculate the summary statistics, measures of central tendency, and better understand our data overall.
"""

print("Rows in the dataset:", data.shape[0])
print("Columns in the dataset:", data.shape[1])

# Daraframe info: not-null count and datatypes
data.info()

"""As we can observe from the output above, our dataset has the following data types for the columns
- Object:
  - Gender
  - ScheduledDay
  - AppointmentDay
  - Neighborhood
  - No-show
- Numerical:
  - PatientID
  - AppointemntID
  - Age
  - Scholarship
  - Hypertension
  - Diabetes
  - Alcoholism
  - Handicap
  - SMS received
"""

# Descriptive Statistics for our Dataset
data.describe()

"""From the descriptive statistics, we can observe the following information about our data
- The average age of patients in our dataset is 37 years old
  - The minimum age is -1, which is an invalid age. This indicates data cleaning will need to be doen to ensure we have only rows with valid ages.
- About 7% of the appointments scheduled are by patients with diabetes
- About 3% of the appointments scheduled are by patients who struggle with alcoholism
- About 32% of the appointments received an SMS message prior to their appointments.
"""

# First 10 rows of our data
data.head(10)

"""Now that we have a general overview of our data, we will dive deeper into the individual variables to ensure that all of our data is properly represented. We will clean our data, rename some of the columns to their correpsonding English spellings (for interpretability), and we will also convert some of the datatypes to make them ready to be processed.

## 2.4 Renaming Columns

Now that we have completed an initial exploration of our data, we will start by correcting the names of our variables, ensuring that they are correctly spelled for better interpretability.

Since ScheduledDay and AppointmentDay are actually dates, we will rename these columns approproately. We will also fix the spelling for Handicap and correct the spellings for Hypertension and Neighborhood to align with American English.

There is a column called "Scholarship" in our data, representing the concept of [Bolsa Família](https://en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia). Bolsa Família is a social welfare program, designed to reduce poverty by providing financial aid to low-income families. It is a conditional cash transfer program, so families need to meet income requirements to receive the aid.  Since the variable name Scholarship is somewhat confusing, we will rename it to GovernmentAidRecipient
"""

# Renaming columns (for spelling and interpretability)
data.rename(columns={'Hipertension': 'Hypertension',
                     'Scholarship': 'GovernmentAidRecipient',
                     'Neighbourhood': 'Neighborhood',
                     'ScheduledDay': 'ScheduledDate',
                     'AppointmentDay': 'AppointmentDate',
                     'SMS_received': 'SMSReceived',
                     'PatientId': 'PatientID',
                     'Handcap': 'Handicap'}, inplace=True)

"""## 2.5 Changing Variable Types

In order to explore our data properly and create any models, we need to make sure that our column datatypes align with our understanding of what they should be. We will start by changing the ScheduledD
"""

# Change Appointment Day/Dates:

# Add column Scheduled Time [will later be converted to morning/afternoon]
data['ScheduledTime'] = pd.to_datetime(data['ScheduledDate']).dt.time
data['ScheduledDate'] = pd.to_datetime(data['ScheduledDate']).dt.date
data['AppointmentTime'] = pd.to_datetime(data['AppointmentDate']).dt.time
data['AppointmentDate'] = pd.to_datetime(data['AppointmentDate']).dt.date
# Add column called Scheduled Day to determine the day of the week of the appointment
data['ScheduledDay'] = pd.to_datetime(data['ScheduledDate']).dt.day_name()
data['AppointmentDay'] = pd.to_datetime(data['ScheduledDate']).dt.day_name()

"""## 2.6 Feature Engineering Part 1: DaysBetween: Days between ScheduledDate and AppointmentDate

Based on our existing variables, we will now create a new variable that reflects the number of days in between our appointments: DaysBetween denotes the number of days that elapsed between when the individual scheduled the appointment to when the appointment actually was.

We suspect, based on additional reading about medical appointment no-shows, that the number of days in between when a person scheduled the appointment and the appointment date are could impact whether they show up to their appointment. We create the new variable below.
"""

# Feature Engineering: DaysBetween
data['DaysBetween'] = (data['AppointmentDate'] - data['ScheduledDate'])

"""We will now transform values in the DaysBetween column to only contain the day integer"""

data['DaysBetween'] = data['DaysBetween'].apply(lambda x: x.days)

"""## 2.7 Converting Handicap Variable into a Binary Variable

We will now change the Handicap column to be a binary variable so that we can better understand whether an individual patient is handicapped or not, as opposed to their specific extremeity of being handicapped.
"""

# Observing the Handicap attribute
data['Handicap'].value_counts()

"""Our data has levels of handicaped indiviudals from 1 to 4, so we will caste the values 2-4 to be 1, so that we can treat Handicap as a binary variable"""

# Converting Handicap values >=1 to 1
data.loc[data['Handicap'] >= 1, 'Handicap'] = 1

data['Handicap'].value_counts()

"""## 2.8 Dropping Null and Invalid Rows

We will now drop the NULL and invalid rows in our data.
"""

num_na_rows = data.isna().any(axis=1).sum()
print(f"Number of rows with missing values: {num_na_rows}")
# drop all null rows
data.dropna(inplace=True)

"""There are no rows with missing values, but we still have some rows that we need to examine closely based on our initial exploration of the data. We will start by observing the age variable, since we previously saw a minimum age value of -1, which doesn't make sense."""

# Determine number of rows with age less than 0
rows_with_negative_age = data[data['Age'] < 0].shape[0]
print(f"Number of rows with age less than 0: {rows_with_negative_age}")

data = data[data['Age'] >= 0]

"""## 2.9 Dropping Duplicate Rows

We will drop the rows in our dataset that are duplicated so that we know we are exploring rows for each real appointment, without double-counting appointments.
"""

# Entire rows that are duplicated:
data.duplicated().sum()

"""There are no rows that are entirely duplicated, but there might be rows that are hidden duplicates. This is likely due to the AppointmentIDs being flagged as unique.

In other words, there might be rows that have the same PatientID, AppointmentDate, ScheduledDate, and No-show status. These rows would represent the same actual doctor's appointment (duplicated appointment records), but these would not be flagged immediately as duplicate rows due to having different documented IDs. We want to remove these rows.
"""

duplicate_rows = data.duplicated(subset=['PatientID', 'AppointmentDate', 'ScheduledDate', 'ScheduledTime', 'No-show'])

# Count the number of duplicate rows
duplicate_count = duplicate_rows.sum()
duplicate_count

data.shape

"""As we can see, there are 1,305 duplicated rows (rows that have the same PatientID, AppointmentDate, ScheduledDate, ScheduledTime, and No-show status). Therefore, we want to drop these rows so that we only have unique appointments in our data.

Our data has levels of handicaped indiviudals from 1 to 4, so we will caste the values 2-4 to be 1, so that we can treat Handicap as a binary variable
"""

data.drop_duplicates(subset=['PatientID', 'AppointmentDate', 'ScheduledDate', 'ScheduledTime', 'No-show'], keep='first', inplace=True)

data.shape

data.head()

"""Our data now contains 109,221 rows with 19 columns. We are now ready for Exploratory Data Analysis.

## 2.10 Observing Presence/Absence of Outliers
We will now observe whether there are any outliers in our data.
"""

data

import numpy as np

# Select only numeric columns
numeric_data = data.select_dtypes(include=np.number)

# Drop specified columns
columns_to_drop = ['PatientID', 'AppointmentID', 'GovernmentAidRecipient', 'Diabetes', 'Alcoholism', 'Handicap']
numeric_data = numeric_data.drop(columns=columns_to_drop, errors='ignore')

# Compute Z-scores for remaining numeric columns
z_scores = np.abs((numeric_data - numeric_data.mean()) / numeric_data.std())

# Threshold: 3
threshold = 3
outliers = (z_scores > threshold)

# Get a list of columns with outliers
outlier_columns = outliers.any().loc[lambda x: x].index.tolist()

# Print the columns with outliers
print("Columns with outliers:")
for column in outlier_columns:
    print(column)

outlier_rows_for_age = data[outliers['Age']]
print("\nOutlier rows for column 'Age':")
print(outlier_rows_for_age)

"""There appear to be two individuals who are 115 years old in our data. Those are our two outlier values."""

outlier_rows_for_days_between = data[outliers['DaysBetween']]
print("\nOutlier rows for column 'DaysBetween':")
print(outlier_rows_for_days_between)

"""Outlier Analysis for DaysBetween:

2,604 rows were identified as outliers for the DaysBetween variable using a z-score threshold of 3. These rows represent appointments scheduled significantly far in advance or with unusually long lead times. We believe that this actually may give us important inofrmation about scheduling behavior, so we have decided to keep these values for further analysis.

## SUMMARY:

In the data loading and preprocessing stage, we imported the necessary libraries, such as pandas and numpy, to handle data manipulation and transformation. We loaded the dataset into a pandas DataFrame and conducted an initial analysis to examine the structure, column types, and distributions of the data. We renamed columns for clarity and consistency and adjusted variable types as needed, such as converting categorical variables to category type and date columns to datetime type. We handled missing or invalid values by dropping rows with null entries and removing any duplicates to ensure the dataset was clean and suitable for model training.

# Part 3: Exploratory Data Analysis (EDA)

We will begin our Exploratory Data Analysis by observing the variables in our dataset. We will start with a simple histogram of the variables in our data. We will then observing the Patients and move on to observing how the variables relate to the target variable, whether the patient did not show up to their appointment.
"""

data.hist(figsize=(20,10))

# Unique patients in our dataset
data.PatientID.nunique()

# Average appointments per patient
len(data) / data.PatientID.nunique()

"""As we can see, there are 62,298 unique patients that scheduled appointments, meaning that on average, each patient scheudled 1.77 apointments"""

data.PatientID.duplicated().sum()

"""There are 46,923 patients that have made more than 1 appointment"""

data.duplicated(['PatientID', 'No-show']).sum()

"""There are 37,405 patients that have had greater than 1 no-show after scheduling an appointment"""

# Different AppointmentTimes in our Data
data['AppointmentTime'].value_counts()

"""Upon inspecting our data, we can observe that the AppointmentTime is 00:00:00 for all appointments, meaning that the column is not giving us any helpful information. Therefore, we will drop this column later."""

# Different ScheduledTimes in our Data
data['ScheduledTime'].value_counts()

"""## 3.1 Plotting and Observing Variables

Below, we will be observing the different variables that are present in our dataset. We will show observe the variables, create a visualization for each, and note the patterns present

### 3.1.1 Understanding Our Target Variable: Class Imbalance

To understand whether our data is imbalanced, we will plot the instances of positively classified data vs negatively classified data: appointment no-shows (1) vs shows (0).
"""

data['No-show'].value_counts()

counts = data['No-show'].value_counts()
counts["Yes"]/(counts["Yes"]+counts["No"])

"""As we can see from the numbers above, there is a clear imbalance in our data, with 20% of no shows among patients. We will plot this imbalance as well."""

# Plot the counts
counts.plot(kind='bar', color=sns.color_palette("rocket", 2), legend=False)
plt.xlabel("No-show")
plt.ylabel("Count")
plt.title("No-shows vs Shows Counts")

"""**Figure1**: This bar chart compares the total counts of "No-shows" (people who missed their appointments) versus "Shows" (people who attended their appointments). The chart highlights an imbalance, where most scheduled appointments were not attended, possibly pointing to an issue that could be further analyzed to identify factors contributing to no-shows.

### 3.1.2 Age
We will now observe the age distribution of our data.
"""

# Set the figure size for better visualization
plt.figure(figsize=(7, 5))
# Plot a kernel density estimate (KDE) to show the distribution of the "Age" variable
sns.kdeplot(data=data, x="Age", hue="No-show", fill=True, palette="rocket", alpha=0.6)

# Add labels and title
# Add a descriptive title for the plot
plt.title("Age Distribution by No-show Status", fontsize=10, fontweight="bold")
plt.xlabel("Age", fontsize=10)
plt.ylabel("Density", fontsize=10)
# Add a legend to distinguish between "No" and "Yes" in the "No-show" category
plt.legend(title="No-show", labels=["Yes", "No"])
# Add a grid to the y-axis for easier interpretation of density values
plt.grid(axis="y", linestyle="--", alpha=0.5)

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()
# Display the plot
plt.show()

"""**Figure 2:** is a Kernel Density Estimate (KDE) plot to visualize the age distribution for individuals grouped by their No-show status (whether they showed up for their medical appointment or not).Younger patients (particularly children) are the most represented group for appointments. While no-shows exist across all ages, they seem to mirror the general distribution of appointments but at a lower scale. Both distributions gradually decline as age increases, with very few patients in the older age groups (above 80)."""

plotting_data = data.copy()
bins = [0, 18, 35, 50, 65, 100]
labels = ["0-18", "19-35", "36-50", "51-65", "65+"]
plotting_data["AgeGroup"] = pd.cut(plotting_data["Age"], bins=bins, labels=labels)

# Calculate no-show rates by age group
no_show_rates = plotting_data.groupby("AgeGroup")["No-show"].value_counts(normalize=True).unstack() * 100

# Plot no-show rates
no_show_rates.plot(kind="bar", stacked=True, figsize=(7, 5), color=sns.color_palette("rocket", 2))

# Add labels and title
# Add a descriptive title for the plot
plt.title("No-show Rates by Age Group", fontsize=14, fontweight="bold")
plt.xlabel("Age Group", fontsize=14)
plt.ylabel("Percentage (%)", fontsize=14)
# Add a legend to distinguish between "No" and "Yes" in the "No-show" category
plt.legend(title="No-show", labels=["No", "Yes"])
plt.xticks(rotation=0)
plt.grid(axis="y",linestyle="--", alpha=0.5)

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()
# Display the plot
plt.show()

"""**Figure 3:** This image shows a stacked bar chart titled "No-show Rates by Age Group". The chart visualizes the percentage of people who either showed up ("No-show: No") or did not show up ("No-show: Yes") for an appointment, categorized by different age groups. This chart suggests that age does not significantly affect appointment attendance, as the "No" category (those who showed up) consistently outweighs the "Yes" category (no-shows) across all groups.

### 3.1.3 Gender
We will now observe the gender distribution present in our data.
"""

# Calculate no-show rates by gender group
plt.figure(figsize=(7, 5))
sns.countplot(data=data, x="Gender", hue="No-show", palette="rocket")

# Add labels and title
# Add a descriptive title for the plot
plt.title("Gender by No-show Status", fontsize=16, fontweight="bold")
plt.xlabel("Gender", fontsize=14)
plt.ylabel("Count", fontsize=14)

# Add a legend to distinguish between "No" and "Yes" in the "No-show" category
plt.legend(title="No-show", labels=["No", "Yes"])

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()
# Display the plot
plt.show()

"""**Figure 4:** This image is a bar chart titled "Gender by No-show Status" that shows the count of individuals who either showed up or did not show up for an appointment, categorized by gender.There are more total appointments associated with females compared to males, regardless of no-show status.
Both genders exhibit a higher count of "No-shows: No" (those who attended) than "No-shows: Yes" (those who missed their appointments), indicating that most individuals keep their appointments.
Females contribute more to the overall count of no-shows compared to males, possibly due to their higher appointment volume.
"""

# TODO:
gender_plot = data.groupby('Gender')['No-show'].apply(lambda x: x.map({'Yes': 1, 'No': 0}).mean()).plot(
    kind='bar', color=sns.color_palette("rocket", 2), figsize=(8, 6), title='No-show Rate by Gender', ylabel='No-show Rate', xlabel='Gender'
)
plt.show()

"""**Figure 4:** (2nd option) This visualization is used to explore whether gender has any noticeable influence on no-show behavior. From this chart, we can see that males show up more than males. The chance of males missing not showing up to an appointment is comparitively less.

### 3.1.4 Hypertension
We will now observe the gender distribution present in our data.
"""

# Map No-show to numeric values for plotting
data_copy = data.copy()

# Add the 'No-show_numeric' column based on 'No-show' values
data_copy['No-show_numeric'] = data_copy['No-show'].map({'No': 0, 'Yes': 1})
# Create a crosstab to calculate counts between Hypertension and No-show
hypertension_no_show = pd.crosstab(data['Hypertension'], data_copy['No-show_numeric'], normalize='index') * 100

# Create a 1x2 grid for subplots (side by side)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# First plot: Violin plot
# The 'data' parameter is now included, specifying the DataFrame to use
sns.violinplot(x='Hypertension', y='No-show_numeric', data=data_copy, palette="rocket", ax=axes[0]) #data parameter is added here
axes[0].set_title("No-show Status by Hypertension (Violin Plot)", fontsize=14, fontweight="bold")
axes[0].set_xlabel("Hypertension (0 = No, 1 = Yes)", fontsize=14)
axes[0].set_ylabel("No-show (0 = No, 1 = Yes)", fontsize=14)

# Second plot: Heatmap
sns.heatmap(hypertension_no_show, annot=True, cmap="Blues", cbar_kws={'label': 'Percentage (%)'}, fmt=".2f",
            linewidths=0.5, linecolor='black', ax=axes[1])
axes[1].set_title("Propotion of No-show Status by Hypertension (Heatmap)", fontsize=14, fontweight="bold")
axes[1].set_xlabel("No-show (0 = No, 1 = Yes)", fontsize=14)
axes[1].set_ylabel("Hypertension (0 = No, 1 = Yes)", fontsize=14)

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()

# Display the plots
plt.show()
data.head()

"""**Figure 5:** This image is a violin plot titled "No-show Status by Hypertension", which visualizes the distribution of no-show status (whether individuals attended their appointments or not) based on hypertension status. Both individuals with and without hypertension are more likely to show up for their appointments than to miss them.
The distribution of no-shows appears similar for both groups, though individuals without hypertension might have a slightly higher density of missing their appointments (visible as a thicker area near 1).

### 3.1.5 Diabetes
We will now observe the distribution of the diabetes variable among the patient appointments in our data.
"""

# Create a figure with 1 row and 2 columns for side-by-side plots
plt.figure(figsize=(14, 5))

# Plot 1: No-show Status by Diabetes
plt.subplot(1, 2, 1)  # First subplot (1 row, 2 columns, first plot)
sns.countplot(data=data, x="Diabetes", hue="No-show", palette="rocket")

# Customize labels and title
plt.title("No-show Status by Diabetes", fontsize=16, fontweight="bold")
plt.xlabel("Diabetes (0 = No, 1 = Yes)", fontsize=14)
plt.ylabel("Count", fontsize=14)
plt.legend(title="No-show", labels=["No", "Yes"], fontsize=12)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Plot 2: Proportion of No-show Status by Alcoholism
plt.subplot(1, 2, 2)  # Second subplot (1 row, 2 columns, second plot)
alcoholism_counts = pd.crosstab(data['Diabetes'], data['No-show'], normalize="index") * 100
sns.heatmap(alcoholism_counts, annot=True, cmap="Blues", cbar_kws={'label': 'Percentage (%)'}, fmt=".1f")

# Add labels and title
plt.title("Proportion of No-show Status by Diabetes", fontsize=16, fontweight="bold")
plt.xlabel("No-show Status", fontsize=14)
plt.ylabel("Diabetes(0 = No, 1 = Yes)", fontsize=14)

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()

# Display the plots
plt.show()

"""**Figure 6:** This bar chart examines the relationship between diabetes status and no-show behavior.This indicates that diabetic patients have lower overall counts in the dataset but show similar attendance patterns to non-diabetic patients.

### 3.1.6 Alcoholism
We will now observe the distribution of the alcoholism variable among the patient appointments in our data.
"""

# Calculate no-show rates by alcoholism group
alcoholism_counts = pd.crosstab(data['Alcoholism'], data['No-show'], normalize="index") * 100

# Create a 1x2 grid for subplots (side by side)
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# First plot: Bar plot
alcoholism_counts.plot(kind="bar", stacked=True, ax=axes[0], color=sns.color_palette("rocket", 2))
axes[0].set_title("Alcoholism by No-show Status (Bar Plot)", fontsize=16, fontweight="bold")
axes[0].set_xlabel("Alcoholism (0 = No, 1 = Yes)", fontsize=14)
axes[0].set_ylabel("Percentage (%)", fontsize=14)
axes[0].legend(title="No-show", labels=["No", "Yes"])

# Second plot: Heatmap
sns.heatmap(alcoholism_counts, annot=True, cmap="Blues", cbar_kws={'label': 'Percentage (%)'}, fmt=".2f",
            linewidths=0.5, linecolor='black', ax=axes[1])
axes[1].set_title("Propotion of Alcoholism by No-show Status (Heatmap)", fontsize=16, fontweight="bold")
axes[1].set_xlabel("No-show Status (0 = No, 1 = Yes)", fontsize=14)
axes[1].set_ylabel("Alcoholism (0 = No, 1 = Yes)", fontsize=14)

# Automatically adjust spacing to prevent overlapping elements
plt.tight_layout()

# Display the plots
plt.show()

"""**Figure 7:**  From the chart, it seems that the percentage of patients showing up or not showing up for appointments is fairly similar across alcoholism groups, indicating no strong correlation between alcoholism and no-show status.

### 3.1.7 SMS Received
We will now observe the distribution of the SMS Received variable among the patient appointments in our data.
"""

# Calculate no-show rates by SMS received group
SMSReceived = pd.crosstab(data['SMSReceived'], data['No-show'], normalize="index") * 100

# Create a figure with 1 row and 2 columns for subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Pie chart for each SMS received status
for i, sms_status in enumerate(SMSReceived.index):
    SMSReceived.loc[sms_status].plot(
        kind='pie',
        autopct='%1.1f%%',
        startangle=90,
        colors=sns.color_palette("rocket", 2),
        labels=["No", "Yes"],
        legend=False,
        ax=axes[i]
    )
    # Add titles for each pie chart
    axes[i].set_title(f"SMS Received {sms_status}", fontsize=14, fontweight="bold")
    axes[i].set_ylabel("")

# Add an overall title for the figure
fig.suptitle("Proportion of No-show by SMS Received Status", fontsize=16, fontweight="bold")

# Automatically adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

"""**Figure 8:** The pie charts depicts the proportion of no-show appointments (Yes/No) for cases where no SMS reminders were received, with 83.4% showing up and 16.6% not showing up.This pie chart depicts the proportion of no-show appointments (Yes/No) for cases where SMS reminders were received, with 72.5% showing up and 27.5% not showing up. **It seems there is a slight negative correlation between receiving SMS and showing up to your appointment**

### 3.1.8 Government Aid
We will now observe the distribution of the GovernmentAidReceived variable among the patient appointments in our data.
"""

# Calculate no-show rates by GovernmentAid group
gov_aid_no_show = pd.crosstab(data['GovernmentAidRecipient'], data['No-show'])

# Plot bar plot
gov_aid_no_show.plot(kind='bar',color=sns.color_palette("rocket", 2), figsize=(7, 5), edgecolor='black')

# Add title and labels
plt.title("Bar Plot of No-show Status by Government Aid Recipient Status", fontsize=16, fontweight="bold")
plt.xlabel("Government Aid", fontsize=14)
plt.ylabel("Count", fontsize=14)

# Add a legend
plt.legend(title="No-show", fontsize=12)

# Automatically adjust spacing
plt.tight_layout()

# Display the plot
plt.show()

"""**Figure 9:** The plot suggests that government aid recipients and non-recipients differ in their no-show behavior, with non-recipients contributing the majority of observations.

### 3.1.9 Handicap
We will now observe the distribution of the Handicap variable among the patient appointments in our data.
"""

# Set up the figure with 1 row and 2 columns
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Create the count plot on the first axis
sns.countplot(data=data, x='Handicap', hue='No-show', palette='rocket', ax=axes[0])
axes[0].set_title('No-show vs Handicap (Count Plot)', fontsize=16)
axes[0].set_xlabel('Handicap (0 = No, 1 = Yes)', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)

# Create the pivot table for the heatmap
pivot_table = pd.crosstab(data['Handicap'], data['No-show'])

# Create the heatmap on the second axis with cmap='Blues'
sns.heatmap(pivot_table, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[1])
axes[1].set_title('Propotion of Handicap by No-show status (Heatmap)', fontsize=16)
axes[1].set_xlabel('No-show (0 = No, 1 = Yes)', fontsize=12)
axes[1].set_ylabel('Handicap (0 = No, 1 = Yes)', fontsize=12)

# Adjust the layout to avoid overlap
plt.tight_layout()

# Show the plots
plt.show()

"""**Figure 10:** The majority of individuals (with or without a handicap) appear to have shown up for their appointments, but the absolute count of individuals without a handicap is much higher.

### 3.1.10: DaysBetween
"""

# Calculate proportions of No-show for each DaysBetween value
proportion_data = (
    data.groupby(['DaysBetween', 'No-show'])
    .size()
    .unstack(fill_value=0)  # Pivot the data for easier calculation
)
proportion_data['Proportion_No-show'] = proportion_data['Yes'] / (proportion_data['Yes'] + proportion_data['No'])

# Reset index to make it easier to plot
proportion_data = proportion_data.reset_index()

plt.figure(figsize=(12, 6))

# Scatter plot of proportions
scatter = plt.scatter(
    x=proportion_data['DaysBetween'],
    y=proportion_data['Proportion_No-show'],
    c=proportion_data['Proportion_No-show'],  # Use proportion for color
    cmap='rocket',  # Apply the rocket palette
    s=proportion_data['Proportion_No-show'] * 200,  # Scale size by proportion
    alpha=0.8
)

# Add color bar
plt.colorbar(scatter, label='Proportion of No-shows')

# Customize plot aesthetics
plt.title('Scatter Plot of Proportion of No-shows vs. Days Between Scheduling and Appointment', fontsize=16)
plt.xlabel('Days Between Scheduling and Appointment', fontsize=12)
plt.ylabel('Proportion of No-shows', fontsize=12)
plt.ylim(0, 1)  # Proportions are between 0 and 1
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()

"""Figure 11: This scatter plot depicts the proportion of no-shows versus the number of days between scheduling and appointment. The x-axis represents the delay in days, while the y-axis shows the no-show proportion, ranging from 0 to 1. Each dot represents a specific data point, with color intensity indicating the no-show proportion based on the color bar on the right. The plot reveals that shorter delays (under 50 days) generally have no-show rates between 20% and 40%, while longer delays (over 100 days) show higher no-show rates, with some extreme cases approaching 100%. This suggests that longer wait times between scheduling and appointment significantly increase the likelihood of no-shows.

### **IMPORTANT TRENDS TO NOTICE:**


*   High No-show Rates: Many appointments are missed, signaling potential systemic issues like scheduling challenges or lack of engagement.
*   SMS Reminders: Surprisingly, receiving SMS reminders correlates with a higher no-show rate, which might indicate messaging inefficiency.
*   Age and No-shows: Younger patients, especially children, dominate the dataset, but age does not significantly influence no-show rates.
*   Gender: Females have more appointments and contribute more to no-shows, likely due to their higher appointment volume.
*   Health Conditions: No-shows are consistent across hypertension, diabetes, and alcoholism groups, with no strong correlations.
*   Economic Factors: Government aid recipients exhibit slightly different no-show patterns, potentially hinting at socioeconomic influences.

## 3.2 Correlation Matrix:

We will now observe whether any of our variables are highly correlated. We will do so by creating a heatmap of our variables. We would want to remove any highly correlated variables if they exist. By definition, highly correlated features are those that have correlations of 0.7 and above. We will now create a heatmap to observe the correlations in our variables to understand the relationships between features and the target variable. We will not be including PatientID or AppointmentID in this heatmap, since these variables should not be included in our heatmap.
"""

heatmap_data = data.select_dtypes(include=['number'])
heatmap_data = heatmap_data.drop(['PatientID', 'AppointmentID'], axis=1)
correlation_matrix = heatmap_data.corr()
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Feature Correlation Heatmap")
plt.show()

"""As we can see, Age and hypertension are the highest correlated variables with a correlation of 0.50, followed by hypertension and diabetes, with a correlation of 0.43. Among hthe other variables, there are not very many correlations present. Since high correlation is defined as being between 0.7 and 1.0, there are no highly correlated variables present in our data. Therefore, we are not dropping the above columns.

Now that we have completed our Exploratory Data Analysis, we will move onto actually creating machine learning models to try to predict a medical appointment no-show. Before doing so, we will prepare our dataframe for analysis, keeping the columns that actually make sense. This will all be done in the Feature Engineering and Preprocessing portion below.

## 3.3 Feature Engineering Part 2: Creating Zipcodes Variable from Neighborhoods

We will now observe the neighborhoods feature in our dataset. We aim to gather zipcodes and coordinates for each neighborhood, and create an interactive map which shows the relationship between neighborhoods and other features

1.   We gather the Zipcodes for each neighborhood using a free publicly available api
2.   We fill in missing Zipcodes manually using a database
3.   We use these zipcodes to gather Latitude and Longitude coordinates
4.   Create interactive map plotting the neighborhoods
"""

counts = data['Neighborhood'].value_counts()

# Print all Neighborhoods and # of patients records
print(counts.to_string())

"""Next, we used the viacep Brazil api which is a Free high-performance webservice for consulting Brazilian Postal Codes to gather the zipcodes for each neighborhood





"""

import pandas as pd
import requests

# List of unique neighborhoods
unique_neighborhoods = data['Neighborhood'].unique()


results = {neighborhood: None for neighborhood in unique_neighborhoods}

# Loop through each neighborhood and stop when one zipcode is found
for neighborhood in unique_neighborhoods:
#for city, state in cities_states:
    # URL for ViaCEP API
    url = f"https://viacep.com.br/ws/ES/Vitória/{neighborhood}/json/"
    try:
        # Send a GET request to the API
        print(f'{neighborhood}')
        response = requests.get(url)
        response.raise_for_status()
        data_return = response.json()
        city = "Vitória"
        state = "ES"
        # Check if valid data is returned
        if isinstance(data_return, list) and len(data_return) > 0:
            # Extract the first result's postal code
            zipcode = data_return[0].get('cep', 'N/A')
            print(f'Found zipcode for {neighborhood}: {zipcode}')
            # Store the result (zipcode, city, state) and move to the next neighborhood
            results[neighborhood] = (zipcode, city, state)
            #break  # Stop searching for this neighborhood once a zipcode is found

    except requests.exceptions.RequestException as e:
        print(f"Error fetching data for {neighborhood} in {city}, {state}: {e}")

# Create a new DataFrame from the results
zipcode_df = pd.DataFrame(
    [
        {
            "Neighborhood": neighborhood,
            "Zipcode": data[0] if data else "NA",
            "City": data[1] if data else "NA",
        }
        for neighborhood, data in results.items()
    ]
)



# Display the new DataFrame
zipcode_df

zipcode_df[zipcode_df['Zipcode'] == 'NA'].value_counts().count()
#47 not found

"""47 Neighborhoods were unable to be mapped to a Zipcode. Below we show these neighborhoods and their patient id counts"""

#How many rows map to these?
na_neighborhoods = zipcode_df[zipcode_df['Zipcode'] == 'NA']['Neighborhood']
na_counts = data[data['Neighborhood'].isin(na_neighborhoods)].groupby('Neighborhood').size()
na_counts.sort_values(ascending=False)

"""In an effort to curate as many Zipcodes as we can, we manually checked the remaining Zipcodes using:
https://www.nigeriapostcodes.ng/region4/brazil/br/vitoria/1037102618/
"""

#Add manual data to DF
neighborhood_data = [
    {"Neighborhood": "JARDIM CAMBURI", "Zipcode": "29090-070", "City": "Vitória"},
    {"Neighborhood": "JARDIM DA PENHA", "Zipcode": "29050-000", "City": "Vitória"},
    {"Neighborhood": "ITARARÉ", "Zipcode": "29047-647", "City": "Vitória"},
    {"Neighborhood": "CENTRO", "Zipcode": "29015-005", "City": "Vitória"},
    {"Neighborhood": "SANTA MARTHA", "Zipcode": "29046-503", "City": "Vitória"},
    {"Neighborhood": "JESUS DE NAZARETH", "Zipcode": "29052-040", "City": "Vitória"},
    {"Neighborhood": "CARATOÍRA", "Zipcode": "29025-645", "City": "Vitória"},
    {"Neighborhood": "JABOUR", "Zipcode": "29072-255", "City": "Vitória"},
    {"Neighborhood": "ILHA DO PRÍNCIPE", "Zipcode": "29020-380", "City": "Vitória"},
    {"Neighborhood": "GURIGICA", "Zipcode": "29046-053", "City": "Vitória"},
    {"Neighborhood": "ILHA DE SANTA MARIA", "Zipcode": "29051-050", "City": "Vitória"},
    {"Neighborhood": "FORTE SÃO JOÃO", "Zipcode": "29017-174", "City": "Vitória"},
    {"Neighborhood": "SÃO CRISTÓVÃO", "Zipcode": "29048-550", "City": "Vitória"},
    {"Neighborhood": "REDENÇÃO", "Zipcode": "29032-785", "City": "Vitória"},
    {"Neighborhood": "JOANA D´ARC", "Zipcode": "29048-070", "City": "Vitória"},
    {"Neighborhood": "PRAIA DO SUÁ", "Zipcode": "29052-330", "City": "Vitória"},
    {"Neighborhood": "GRANDE VITÓRIA", "Zipcode": "29031-307", "City": "Vitória"},
    {"Neighborhood": "INHANGUETÁ", "Zipcode": "29031-610", "City": "Vitória"},
    {"Neighborhood": "ILHA DAS CAIEIRAS", "Zipcode": "29032-131", "City": "Vitória"},
    {"Neighborhood": "PRAIA DO CANTO", "Zipcode": "29055-285", "City": "Vitória"},
    {"Neighborhood": "BENTO FERREIRA", "Zipcode": "29050-620", "City": "Vitória"},
    {"Neighborhood": "VILA RUBIM", "Zipcode": "29025-023", "City": "Vitória"},
    {"Neighborhood": "DO QUADRO", "Zipcode": "29027-110", "City": "Vitória"},
    {"Neighborhood": "MONTE BELO", "Zipcode": "29053-325", "City": "Vitória"},
    {"Neighborhood": "JUCUTUQUARA", "Zipcode": "29040-750", "City": "Vitória"},
    {"Neighborhood": "FONTE GRANDE", "Zipcode": "29016-714", "City": "Vitória"},
    {"Neighborhood": "MATA DA PRAIA", "Zipcode": "29066-160", "City": "Vitória"},
    {"Neighborhood": "ESTRELINHA", "Zipcode": "29031-012", "City": "Vitória"},
    {"Neighborhood": "SANTA LÚCIA", "Zipcode": "29056-055", "City": "Vitória"},
    {"Neighborhood": "SANTA LUÍZA", "Zipcode": "29045-410", "City": "Vitória"},
    {"Neighborhood": "BARRO VERMELHO", "Zipcode": "29057-570", "City": "Vitória"},
    {"Neighborhood": "DO MOSCOSO", "Zipcode": "29020-749", "City": "Vitória"},
    {"Neighborhood": "COMDUSA", "Zipcode": "29032-055", "City": "Vitória"},
    {"Neighborhood": "ARIOVALDO FAVALESSA", "Zipcode": "29027-017", "City": "Vitória"},
    {"Neighborhood": "FRADINHOS", "Zipcode": "29042-327", "City": "Vitória"},
    {"Neighborhood": "ENSEADA DO SUÁ", "Zipcode": "29050-230", "City": "Vitória"},
    {"Neighborhood": "SANTA HELENA", "Zipcode": "29055-090", "City": "Vitória"},
    {"Neighborhood": "HORTO", "Zipcode": "29045-157", "City": "Vitória"},
    {"Neighborhood": "UNIVERSITÁRIO", "Zipcode": "29031-540", "City": "Vitória"},
    {"Neighborhood": "SEGURANÇA DO LAR", "Zipcode": "29072-355", "City": "Vitória"},
    {"Neighborhood": "MORADA DE CAMBURI", "Zipcode": "29062-555", "City": "Vitória"},
    {"Neighborhood": "PONTAL DE CAMBURI", "Zipcode": "29062-030", "City": "Vitória"},
    {"Neighborhood": "ILHA DO BOI", "Zipcode": "29052-620", "City": "Vitória"},
    {"Neighborhood": "ILHA DO FRADE", "Zipcode": "29057-015", "City": "Vitória"},
    {"Neighborhood": "AEROPORTO", "Zipcode": "29075-600", "City": "Vitória"},
    {"Neighborhood": "ILHAS OCEÂNICAS DE TRINDADE", "Zipcode": "29052-800", "City": "Vitória"},
    {"Neighborhood": "PARQUE INDUSTRIAL", "Zipcode": "29092-300", "City": "Vitória"}
]
manual_update_zipcodes = pd.DataFrame(neighborhood_data)

#Update zipcode_df:
manual_data_dict = manual_update_zipcodes.set_index('Neighborhood').to_dict('index')

# Update zipcodes_df
for index, row in zipcode_df.iterrows():
    neighborhood = row['Neighborhood']
    if neighborhood in manual_data_dict:
        zipcode_df.at[index, 'Zipcode'] = manual_data_dict[neighborhood]['Zipcode']
        zipcode_df.at[index, 'City'] = manual_data_dict[neighborhood]['City']

"""We now strip away the trailing "street" aspect of the Zipcodes, returning a larger area which will include multiple neighborhoods"""

#Use Regex to strip away street id for Zipcodes:
zipcode_df['Zipcode'] = zipcode_df['Zipcode'].str.replace(r'-.*', '', regex=True)

zipcode_df

"""Next, we gather the Latitude and Longitude coordinates based on the Zipcodes using a freely available API and key"""

#Use Free API to get Latitude and Longitude coordinates for Zipcodes
import pandas as pd
import requests
import time

# Load your DataFrame (replace with your actual DataFrame loading code)
# Example:
# OpenCage Geocoder API key (replace with your API key)
OPENCAGE_API_KEY = '2f5c2ee6483d44aba56a9244dc680063'
OPENCAGE_URL = 'https://api.opencagedata.com/geocode/v1/json'

# Function to get latitude and longitude using OpenCage Geocoder
def get_lat_lng(zipcode):
    params = {
        'q': f"{zipcode}, Vitória, ES, Brazil",
        'key': OPENCAGE_API_KEY
    }
    response = requests.get(OPENCAGE_URL, params=params)
    if response.status_code == 200:
        data = response.json()
        if data['results']:
            location = data['results'][0]['geometry']
            return location['lat'], location['lng']
    return None, None

# Add latitude and longitude columns to the DataFrame
manual_update_zipcodes['Latitude'] = None
manual_update_zipcodes['Longitude'] = None

# Loop through each zipcode and fetch latitude/longitude
for idx, row in zipcode_df.iterrows():
    zipcode = row['Zipcode']
    Neighborhood = row['Neighborhood']
    lat, lng = get_lat_lng(zipcode)
    #print(f'Neighborhood {Neighborhood} latitude: {lat} longitude: {lng}')
    zipcode_df.at[idx, 'Latitude'] = lat
    zipcode_df.at[idx, 'Longitude'] = lng
    print(f"Processed {Neighborhood} {zipcode}: Latitude={lat}, Longitude={lng}")
    time.sleep(1)  # Avoid hitting API rate limits

# Save the updated DataFrame to a CSV file if needed
#zipcode_df.to_csv('zipcodes_with_latlng.csv', index=False)

print("Latitude and longitude data added successfully.")

#Show the zipcode_df
zipcode_df

#How many unique zipcodes? How many rows correspond to the zipcodes?
unique_zipcodes = zipcode_df['Zipcode'].unique()
unique_zipcode_count = len(unique_zipcodes)
unique_zipcode_count

"""We have 81 unique neighborhoods, and 34 unique zipcodes meaning that we have multiple neighborhoods in the dataset represented by the same zipcode."""

#Show Zipcodes with the most neighborhoods
zipcode_df.groupby('Zipcode').size().sort_values(ascending=False)

#How many Coordinates overlap?
zipcode_df.groupby(['Latitude', 'Longitude']).size().sort_values(ascending=False)

"""43 of our neighborhoods have the same coordinates"""

#Merge Zipcodes df which contains Zipcode, City, Latitude, Longitude to the main dataframe
updated_data = data.merge(zipcode_df, on='Neighborhood', how='left')

no_show_proportion = (
    updated_data.groupby('Zipcode')['No-show']
    .apply(lambda x: (x == 'Yes').mean())
    .reset_index(name='No-show Proportion')
)

# Sort by Zipcode for a cleaner plot
no_show_proportion = no_show_proportion.sort_values('Zipcode')
no_show_proportion

# Plotting the data
plt.figure(figsize=(12, 6))
plt.bar(no_show_proportion['Zipcode'].astype(str), no_show_proportion['No-show Proportion'])
plt.xticks(rotation=90)
plt.xlabel('Zipcode')
plt.ylabel('Proportion of No-shows')
plt.title('Proportion of No-shows by Zipcode')
plt.tight_layout()
plt.show()

"""This image depicts a bar chart showing the proportion of no-shows by zip code. The x-axis lists various zip codes, while the y-axis represents the no-show rate, ranging between 0.15 and 0.25. Each bar indicates the proportion of no-shows for a specific zip code, highlighting differences in missed appointments across locations. Some zip codes, such as 29020 and 29047, have the highest no-show rates near 0.25, while others, like 29017 and 29066, show lower rates around 0.17. This visualization helps identify areas where no-show rates are higher, which can guide targeted intervention strategies.

**Create an interactive Map of the Zipcodes**
"""

import folium
from folium import Popup
from folium.plugins import MarkerCluster
import pandas as pd

# Step 1: Calculate no-show proportion for each Zipcode
Grouped_no_show_proportion = (
    updated_data.groupby('Zipcode')['No-show']
    .apply(lambda x: (x == 'Yes').mean())
    .reset_index(name='Grouped_No_show_Proportion')
)

# Step 2: Group by 'Zipcode' and calculate required metrics, including 'DaysBetween'
grouped_data = updated_data.groupby('Zipcode').agg(
    Grouped_Average_Age=('Age', 'mean'),
    Grouped_Proportion_SMSReceived=('SMSReceived', 'mean'),
    Grouped_Proportion_GovernmentAid=('GovernmentAidRecipient', 'mean'),
    Patient_Count=('PatientID', 'count'),  # Add patient count
    Grouped_Average_DaysBetween=('DaysBetween', 'mean')  # Add average days between
).reset_index()

# Step 3: Merge grouped data with no-show proportion
grouped_data = grouped_data.merge(Grouped_no_show_proportion, on='Zipcode', how='left')

# Step 5: Check and handle column conflicts before merging
# Identify overlapping columns (except the merge key 'Zipcode')
overlapping_columns = set(zipcode_df.columns) & set(grouped_data.columns) - {'Zipcode'}

# Drop overlapping columns from zipcode_df if they exist
zipcode_df = zipcode_df.drop(columns=overlapping_columns, errors='ignore')

# Merge grouped_data with zipcode_df
zipcode_df = zipcode_df.merge(grouped_data, on='Zipcode', how='left', suffixes=('', '_new'))

# Debugging: Check columns in zipcode_df
print("Zipcode DataFrame Columns:", zipcode_df.columns)

# Step 6: Create the map and add markers
# Center the map on Vitória, Brazil
center_coordinates = [-20.3055, -40.3128]  # Vitória's latitude and longitude
map = folium.Map(location=center_coordinates, zoom_start=14)

# Create a MarkerCluster for better visualization of multiple points
marker_cluster = MarkerCluster().add_to(map)

# Add markers for each row in the dataframe
for idx, row in zipcode_df.iterrows():
    latitude = row['Latitude']
    longitude = row['Longitude']
    neighborhood = row['Neighborhood']
    zipcode = row['Zipcode']
    average_age = row['Grouped_Average_Age']
    proportion_sms_received = row['Grouped_Proportion_SMSReceived'] * 100  # Convert to percentage
    proportion_government_aid = row['Grouped_Proportion_GovernmentAid'] * 100  # Convert to percentage
    no_show_proportion = row['Grouped_No_show_Proportion'] * 100  # Convert to percentage
    average_days_between = row['Grouped_Average_DaysBetween']  # Average days between
    patient_count = row['Patient_Count']  # Patient count

    # Determine color based on No-show Proportion
    if no_show_proportion > 20:  # High no-show proportion
        color = 'red'
    elif no_show_proportion <= 20:  # Low no-show proportion
        color = 'green'
    else:  # Intermediate no-show proportion
        color = 'yellow'

    # Add metrics to the popup content
    popup_content = (
        f"Neighborhood: {neighborhood}<br>"
        f"Zipcode: {zipcode}<br>"
        f"No-show Proportion: {no_show_proportion:.2f}%<br>"
        f"Number of Patients: {patient_count}<br>"  # Add patient count
        f"Average Age: {average_age:.2f}<br>"
        f"SMS Received: {proportion_sms_received:.2f}%<br>"
        f"Government Aid Recipient: {proportion_government_aid:.2f}%<br>"
        f"Average Days Between Appointments: {average_days_between:.2f}<br>"  # Add DaysBetween
    )
    popup = Popup(popup_content, max_width=300)

    # Add marker with color-coded icon
    folium.Marker(
        location=[latitude, longitude],
        popup=popup,
        icon=folium.Icon(color=color)
    ).add_to(marker_cluster)

# Render the map
map

"""## UML Diagram:

To model the relationships in our data, we have created a UML Diagram. This depicts 3 entitites.

![Blank diagram-3.png](data:image/png;base64,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)

# Part 4: Preprocessing Data for Modeling Purposes

For the purpose of creating our models, we will create a copy of our current `data` dataframe, and call it `df`. We will modify `df` such that it includes the variables that are helpful for modeling, now that our EDA process has been completed.
"""

# Create copy of data: save into df
df = updated_data.copy()

df.head()

df.columns

"""We will be dropping the columns that we do not want to include in our model: PatientID, AppointmentID, ScheuledDate, AppointmentDate, ScheduledTime, AppointmentTime, City, Latitude, Longitude, and Neighborhood."""

# Dropping unwanted columns
df.drop(columns=['PatientID', 'AppointmentID', 'ScheduledDate', 'AppointmentDate', 'ScheduledTime', 'AppointmentTime', 'City', 'Latitude', 'Longitude', 'Neighborhood'], inplace=True)

df

"""## 4.2: One-hot Encode Variables

We will be one-hot encoding the following variables:
1. Gender
2. ScheduledDay
3. AppointmentDay

We want to observe whether gender has an impact on whether an individual does not show up to their appointment. Similarly, we suspect that ScheduledDay or AppointmentDay impact whether someone does not show up to their appointments.
"""

# One hot encoding:
columns_to_encode = ['Gender', 'ScheduledDay', 'AppointmentDay', 'Zipcode']
df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)

df.columns

"""The gender column has been converted into Gender_M, and the AppointmentDay of Friday has been dropped.

## 4.3 Appropriately Converting Variables into Binary Variables

We will now convert our no-show column into a binary variable, such that 1 indicates a "Yes" and 0 indicates a "No". This will be our outcome variable that we are trying to predict. We will also ensure that we replace all Trues in our dataframe into 1s, and all Falses into 0s.
"""

# No-show column:
df['No-show'] = df['No-show'].str.strip().str.capitalize()

# Convert 'No-show' to numeric values (Yes → 1, No → 0)
df['No-show'] = df['No-show'].map({'Yes': 1, 'No': 0})

# Check for any remaining NaN values
print(df['No-show'].isna().sum())

# replace all Trues into 1s, and all Falses into 0s
df = df.replace({True: 1, False: 0})

"""## 4.3 Train-Test Split

We will now prepare our data for modeling by splitting our data into X (features) and y (outcome). We will also split our data into train, validation, and test splits. We will also standardize the numeric variables in our data.
"""

df

# Creating X and y from our df
X = df.drop(columns=['No-show'])
y = df['No-show']

"""**SUMMARY:**

In the feature engineering phase, we created two new features: zip codes for the neighborhood and days in between the scheduling day and the appointment day. These new features were added to provide more granular information and improve the model’s ability to make accurate predictions.

Additionally, we performed a test-train split to ensure proper training and evaluation of the model. We also applied one-hot encoding to the Gender, ScheduledDay, and AppointmentDay variables, as we wanted to observe their impact on the no-show rate. Specifically, we aimed to explore whether Gender influences the likelihood of no-shows, and whether ScheduledDay or AppointmentDay affect the chances of a patient missing their appointment. These preprocessing steps helped prepare the dataset for effective model development.

# Part 5: Modeling

# Modeling
For the classification task, the goal is to predict whether a patient will show up for their appointment using all the features provided.

We will be implementing the following 4 models and tuning them after implementing initial versions of the model. For our baseline model, we will be using a standard logistic regression model.

Our models are below:
1. Logistic Regression (Baseline Model):
A simple model to start with, which is effective for binary classification tasks.  It provides a straightforward approach to understanding the relationship between the features (e.g., Age, Hypertension, etc.) and the likelihood of a medical no-show, which is helpful for initial comparisons against more complex models.

2. Decision Tree Classifier:
Decision Trees are easy to interpret and provide clear decision rules for classification, which can be useful for understanding the driving factors behind a no-show. While they are prone to overfitting, they can capture complex relationships between features and are a good starting point for exploring the structure of the dataset.

3. Random Forest Classifier:
Random Forest is an ensemble learning method that improves predictive performance by combining multiple decision trees. It is particularly effective for handling imbalanced datasets because it builds multiple trees and reduces overfitting, thus improving accuracy and generalization. Random Forest is also less sensitive to outliers and missing values, making it robust for real-world data.

4. XGBoost Classifier:
 XGBoost is a gradient boosting algorithm that that works well for imbalanced datasets by focusing on minimizing error in the difficult-to-classify instances. It often performs well in classification tasks with structured data, and ultimately making it a strong candidate for predicting no-show events

In this stage, we split the dataset into training and testing sets using an 80-20 split, ensuring the distribution of the target variable (y) is preserved with stratification. We then identified the numerical columns, Age and DaysBetween, for scaling. Using StandardScaler, we standardized these numerical features to have a mean of 0 and a standard deviation of 1, improving the model's performance by ensuring all features are on a similar scale. The scaling was applied to both the training and testing sets, with the scaler fitted on the training data and then applied to the test data to avoid data leakage.

## 5.1 Train-Test Split, Stratifying y, and Scaling
Below, we will split our data into a training and test set while making sure to stratify on our outcome variable Stratifying on y during our train-test splitting helps us ensure that the class proportions in the training and test sets are the same as in the original dataset, maintaining a representative distribution for reliable model training and evaluation. We will implement a test size of 20% and a train size of 80%.
"""

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Cols to scale
numeric_columns = ['Age', 'DaysBetween']

# Scale numerical features
scaler = StandardScaler()

X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_test_scaled[numeric_columns] = scaler.transform(X_test[numeric_columns])

"""## 5.1 Baseline Logistic Regression

Now that our data is ready for processing, we will be creating a baseline logistic regression model. Since in our EDA step, we observed a difference in our target variable, we have made sure to split our data such that we are stratifying on our target variable (y). When training the logistic regression model, we will balance our class weights as well.

Below, we are training a standard logistic regression model, evaluating its performance based on accuracy, F1-score, and the classification report.
"""

# Train the logistic regression model using only numerical features
logreg = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)
logreg.fit(X_train_scaled, y_train)

# Predict and evaluate Standard :ogistic Regression
y_pred_logreg = logreg.predict(X_test_scaled)
print("Standard Logistic Regularization")
print("Accuracy:", accuracy_score(y_test, y_pred_logreg))
print("F1-Score:", f1_score(y_test, y_pred_logreg))
print(classification_report(y_test, y_pred_logreg))

"""The Standard Logistic Regularization model achieved an accuracy of 66.4%, with an F1-score of 0.40. The performance metrics for class 0 (the majority class) show a high precision of 0.86, recall of 0.69, and F1-score of 0.77. For class 1 (the minority class), the precision is lower at 0.31, but the recall is higher at 0.57, leading to an F1-score of 0.40. The macro average F1-score is 0.58, while the weighted average F1-score is 0.69, reflecting better performance on class 0."""

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred_logreg)

# Create a heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=logreg.classes_, yticklabels=logreg.classes_)
plt.title('Confusion Matrix: Standard Logistic Regression')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""The image shows a confusion matrix for a Standard Logistic Regression model, illustrating its classification performance. The matrix has four key values: 12,034 true negatives (correctly predicted 0), 5,435 false positives (incorrectly predicted 1), 1,898 false negatives (incorrectly predicted 0), and 2,478 true positives (correctly predicted 1). The darker color intensity represents higher counts, emphasizing the distribution of predictions. This matrix provides insight into the model's performance, including metrics like accuracy, precision, recall, and F1-score, which can be derived to assess its classification effectiveness.

## 5.2: Additional Logistic Regression Models

We will now train and evaluate two additional logistic regression models. We had previously trained a standard logistic regression model, but now we will train regularized logistic regression models to observe whether adding a regularization term improves the test accuracy and f1 score: L2 regularization and L1 regularization. Ridge regularization (L2) to the logistic regression, which penalizes large coefficients, and evaluated its performance in the same way. Lastly, we trained a Lasso regularization (L1) logistic regression model, which encourages sparsity by reducing some coefficients to zero, and evaluated its results. For each model, the performance metrics, including accuracy, F1-score, and a detailed classification report, were printed to compare their effectiveness in predicting the target variable.
"""

# Ridge Regularization (L2)
ridge_logreg = LogisticRegression(penalty='l2', class_weight='balanced', random_state=42, max_iter=1000)
ridge_logreg.fit(X_train_scaled, y_train)

# Predict and evaluate Ridge
y_pred_ridge = ridge_logreg.predict(X_test_scaled)
print("Ridge Regularization")
print("Accuracy:", accuracy_score(y_test, y_pred_ridge))
print("F1-Score:", f1_score(y_test, y_pred_ridge))
print(classification_report(y_test, y_pred_ridge))

# Lasso Regularization (L1)
lasso_logreg = LogisticRegression(penalty='l1', solver='saga', class_weight='balanced', random_state=42, max_iter=1000)
lasso_logreg.fit(X_train_scaled, y_train)

# Predict and evaluate Lasso
y_pred_lasso = lasso_logreg.predict(X_test_scaled)
print("\nLasso Regularization")
print("Accuracy:", accuracy_score(y_test, y_pred_lasso))
print("F1-Score:", f1_score(y_test, y_pred_lasso))
print(classification_report(y_test, y_pred_lasso))

"""**Explanation of the Accuracy/Model:**

The model performance for all three logistic regression variations (Standard, Ridge, and Lasso regularization) is similar, with an accuracy of around 66% and an F1-score of approximately 0.40. While the precision for predicting no-shows (class 0) is high at 0.86, the recall for the same class is 0.69, showing that the model is reasonably good at identifying no-shows but still has room for improvement. For the "show" class (class 1), precision is low at 0.31, and recall is 0.57, indicating that the model struggles to correctly identify patients who show up. The macro average F1-score is 0.58, reflecting an overall decent but unbalanced performance, and the weighted average F1-score of 0.69 highlights the model's bias towards the majority class. These results suggest that while the model is effective for predicting no-shows, it needs improvement in predicting shows, especially in imbalanced datasets.

## Additional Baseline Modeling

We will now implement three additional models as explained in the beginning of the project file:  
2. Decision Tree Classifier
3. Random Forest Classifier
4. Extreme Gradient Boosting: XGBoost Classifier

Once we do so, we will observe their performance. Then, we will tune the models.

## 5.2 Decision Tree Classifier

For our second model, we chose the Decision Tree supervised learning model because it efficiently handles both numerical and categorical data with minimal preprocessing. It effectively captures non-linear relationships and feature interactions, while also offering insights into feature importance. Additionally, Decision Trees are widely used for classification tasks, making them a reliable choice for our analysis.
"""

dt_clf_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)
dt_clf_model.fit(X_train, y_train)
y_pred_dt = dt_clf_model.predict(X_test)
print("Decision Tree Classifier")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("F1-Score:", f1_score(y_test, y_pred_dt, average='weighted'))
print(classification_report(y_test, y_pred_dt))

"""This model achieved an accuracy of 71.2% with an F1-score of 0.72. For class 0, the performance was strong, with a precision of 0.83, recall of 0.80, and F1-score of 0.82. However, for class 1, the performance was much weaker, with a precision of 0.31, recall of 0.36, and F1-score of 0.33. The macro average F1-score is 0.57, and the weighted average F1-score is 0.72, indicating that the model performs better on the majority class (class 0) but struggles with the minority class (class 1). The F1-score of 0.72 suggests an overall improvement in handling both classes compared to some earlier models.


"""

cm_dt = confusion_matrix(y_test, y_pred_dt)

# Create a heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=dt_clf_model.classes_, yticklabels=dt_clf_model.classes_)
plt.title('Confusion Matrix: Decision Tree Classifier')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""The Decision Tree Classifier performs well in predicting no-shows (class 0) with an accuracy of 71%, achieving high precision (0.83) and recall (0.80) for this class. However, it struggles with predicting shows (class 1), with low precision (0.31) and recall (0.36), meaning it often incorrectly predicts that patients will not show up. The F1-score for class 1 is also low at 0.33, highlighting the model’s difficulty in correctly identifying the minority class. The macro average F1-score of 0.57 reflects overall moderate performance, while the weighted average of 0.72 shows better results when considering the class imbalance, suggesting the model is biased toward the majority class."""

# Feature Importance
importances = dt_clf_model.feature_importances_
features = X.columns

# Plotting feature importance
plt.figure(figsize=(8, 6))
plt.barh(features, importances, align='center')
plt.xlabel('Feature Importance')
plt.title('Feature Importance in Decision Tree Classifier')

# Adjust y-axis font size
plt.tick_params(axis='y', which='major', labelsize=5)  # Change 8 to your desired font size

plt.show()

"""The image shows a bar chart of feature importance from a decision tree classifier, highlighting the significance of various features in the model's predictions. Among the most impactful features are "ScheduledDay_Monday" and "Age," which contribute the most to the decision-making process. Other features like "Hypertension" and "GovernmentalRecipient" also play moderate roles. In contrast, many zip code and appointment-related features have minimal importance, indicating they contribute little to the model's outcome. This visualization helps pinpoint which features are key drivers of the model's performance.

## 5.3 Random Forest Classifier

For our third model, we utilized a Random Forest classifier, as it enhances predictive accuracy and minimizes overfitting by combining multiple Decision Trees. This model effectively handles both numerical and categorical data while capturing complex feature interactions. It also offers valuable insights into feature importance, helping identify key predictors of patient no-shows. Furthermore, its robustness to noise and missing data makes it an ideal choice for real-world healthcare datasets.
"""

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("Random Forest Classifier")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("F1-Score:", f1_score(y_test, y_pred_rf, average='weighted'))
print(classification_report(y_test, y_pred_rf))

"""The Random Forest Classifier achieved an accuracy of 76.6%, with an F1-score of 0.75. The model performed well for class 0, with a precision of 0.82, recall of 0.90, and F1-score of 0.86. However, for class 1, the performance was weaker, with a precision of 0.37, recall of 0.24, and an F1-score of 0.29. The macro average F1-score is 0.58, while the weighted average F1-score is 0.75, indicating that the model performs much better for class 0 but still faces significant challenges in correctly classifying the minority class (class 1).


"""

cm_rf = confusion_matrix(y_test, y_pred_rf)

# Create a heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', xticklabels=rf_model.classes_, yticklabels=rf_model.classes_)
plt.title('Confusion Matrix: Random Forest Classifier')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""## 5.4 Extreme Gradient Boosting- XGBoost
The third model we selected is Extreme Gradient Boosting (XGBoost), a highly efficient machine learning algorithm tailored for supervised learning tasks such as regression, classification, and ranking problems. XGBoost constructs an ensemble of decision trees through a process of gradient boosting, which iteratively minimizes errors by learning from the mistakes of previous models. This model also incorporates regularization techniques to prevent overfitting and supports missing data handling, making it a powerful and versatile choice for complex predictive modeling tasks.

"""

# Calculate scale_pos_weight
neg_class = (y_train == 0).sum()
pos_class = (y_train == 1).sum()
scale_pos_weight = neg_class / pos_class

# Train XGBoost with scale_pos_weight
xgb_model = XGBClassifier(use_label_encoder=False, scale_pos_weight=scale_pos_weight, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

y_pred_xgb = xgb_model.predict(X_test)

print("XGBoost")
print("Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("F1-Score:", f1_score(y_test, y_pred_xgb, average='weighted'))
print(classification_report(y_test, y_pred_xgb))

"""The XGBoost model achieved an accuracy of 61.5%, with an F1-score of 0.65. For class 0, the model performed well with a precision of 0.91, but the recall was lower at 0.58, resulting in an F1-score of 0.71. For class 1, the precision was 0.31, but the recall was much higher at 0.76, leading to an F1-score of 0.44. The macro average F1-score is 0.57, and the weighted average F1-score is 0.65, indicating better overall performance for class 0. However, the model still struggles with the minority class (class 1), despite the higher recall.

## Finetuning

Now that we have completed our initial 4 models, we will now tune the hyperparameters to observe whether would enable us to have a higher test accuracy. We are tuning all of our models using Randomized Search, as we had observed was a good industry practice in class.

We will go in the following order (same as above)
1. Logistic Regression
2. Decision Tree Classifier
3. Random Forest Classifier
4. XGBoost Classifier

For finetuning, we will use a train-validation-test split.

## 5.5: Tuned Logistic Regression

1. Logistic Regression


*   penalty: Type of regularization (l1 or l2).
Why: Regularization helps prevent overfitting by penalizing large coefficients.Tuning ensures the best type of regularization for the dataset.
*   C: Regularization strength.
Why: Determines how much penalty is applied for large coefficients.
Tuning balances bias (underfitting) and variance (overfitting).

*   solver: Optimization algorithm.
Why: Different solvers support different penalties and datasets.
*   class_weight: Balancing class importance (balanced or None).
Why: Handles imbalanced datasets by giving minority class higher weight.
"""

# Define the hyperparameters distribution for tuning
param_dist = {
    'penalty': ['l1', 'l2'],  # Reduced to only test L1 and L2 penalties
    'C': np.logspace(-2, 0, 3),  # Narrowed range for regularization strength
    'solver': ['saga'],  # Suitable solver for L1 and L2
}

# Initialize the Logistic Regression model
logreg = LogisticRegression(max_iter=500, random_state=42, class_weight='balanced')

# Use RandomizedSearchCV to perform hyperparameter tuning with cross-validation
random_search = RandomizedSearchCV(
    logreg,
    param_distributions=param_dist,
    n_iter=5,  # Fewer combinations to check
    cv=3,  # Reduced to 3-fold cross-validation
    scoring='f1',  # Scoring based on F1-Score
    verbose=1,  # Show progress
    n_jobs=-1,  # Use all available cores for parallel processing
    random_state=42,  # Ensure reproducibility
)

# Train the model on the training set with hyperparameter tuning
random_search.fit(X_train_scaled, y_train)

# Print the best hyperparameters found
print("Best hyperparameters:", random_search.best_params_)

# Use the best model found by RandomizedSearchCV to make predictions
best_logreg = random_search.best_estimator_
y_pred_best_logreg = best_logreg.predict(X_test_scaled)

# Evaluate the model performance
print("Best Logistic Regression Model with Hyperparameter Tuning")
print("Accuracy:", accuracy_score(y_test, y_pred_best_logreg))
print("F1-Score:", f1_score(y_test, y_pred_best_logreg))
print(classification_report(y_test, y_pred_best_logreg))

"""This displays the evaluation results of a Logistic Regression model with hyperparameter tuning, where the best hyperparameters found are solver='saga', penalty='l2', and C=0.01. The model achieves an accuracy of 66.53% and an F1-score of 0.40. For class 0, precision is high (0.86), but recall is moderate (0.69), while for class 1, precision is lower (0.31) and recall is higher (0.56), resulting in an F1-score of 0.40. The overall metrics show that the model performs better on the majority class, with macro and weighted averages of precision, recall, and F1-score indicating some imbalance, especially for class 1.

## 5.6: Tuned Decision Tree Classifier

2. Decision Tree

*   max_depth: Maximum depth of the tree.
Why: Limits overfitting by controlling the complexity of the tree.
*   min_samples_split: Minimum samples required to split an internal node.
Why: Prevents overly deep trees by enforcing splits only when sufficient data is available.

*   class_weight: Balancing class importance.
Why: Corrects for class imbalance by weighting classes during split decisions.
"""

param_dist = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8],
    'class_weight': ['balanced', None],
    'criterion': ['gini', 'entropy']
}

# Initialize the Decision Tree Classifier
dt_clf_model = DecisionTreeClassifier(random_state=42)

# Use RandomizedSearchCV with a broader range and stratified cross-validation
random_search = RandomizedSearchCV(
    dt_clf_model,
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    scoring='f1_weighted',  # Use f1_weighted for imbalanced datasets
    n_jobs=-1,
    verbose=1,
    random_state=42
)

# Train the model with the original unscaled training data
random_search.fit(X_train, y_train)

# Print the best hyperparameters found
print(f"Best hyperparameters: {random_search.best_params_}")

# Use the best model found by RandomizedSearchCV to make predictions
best_dt_clf = random_search.best_estimator_
y_pred_best_dt = best_dt_clf.predict(X_test)

# Evaluate the model performance
print("Best Decision Tree Classifier with Hyperparameter Tuning")
print("Accuracy:", accuracy_score(y_test, y_pred_best_dt))
print("F1-Score:", f1_score(y_test, y_pred_best_dt, average='weighted'))
print(classification_report(y_test, y_pred_best_dt))

"""The Best Decision Tree Classifier with Hyperparameter Tuning achieved an accuracy of 75.9%, with an F1-score of 0.75. For class 0, it showed strong performance with a precision of 0.83, recall of 0.88, and F1-score of 0.85. However, for class 1, the performance was much weaker, with a precision of 0.37, recall of 0.28, and an F1-score of 0.31. The macro average F1-score is 0.58, and the weighted average F1-score is 0.75, indicating that while class 0 is well-handled, the model still struggles with the minority class (class 1), despite the hyperparameter tuning.

## 5.7: Tuned Random Forest Classifier

3. Random Forest


*   n_estimators: Number of decision trees.
Why: More trees improve stability but increase computation time.
*   max_depth: Maximum depth of each tree.
Why: Controls overfitting by limiting complexity.
*   class_weight: Balancing class importance.
Why: Ensures that minority class predictions are not ignored.
*   max_features: Number of features considered for the best split.
Why: Introduces randomness to prevent overfitting and improve generalization.
"""

param_dist = {
    'n_estimators': [50, 100],  # Fewer options for number of trees
    'max_depth': [10, 20, None],  # Focus on a narrower range of depths
    'min_samples_split': [2, 10],  # Reduce options for split criteria
    'min_samples_leaf': [1, 4],  # Reduce options for minimum samples in leaf
    'max_features': ['sqrt'],  # Focus on 'sqrt', a common good default
    'bootstrap': [True],  # Fix to True (common for Random Forest)
    'class_weight': ['balanced'],  # Fix to balanced for imbalanced datasets
    'criterion': ['gini']  # Use only Gini (faster than entropy)
}

# Initialize the Random Forest Classifier
rf_clf_model = RandomForestClassifier(random_state=42)

# Use RandomizedSearchCV with fewer iterations and reduced complexity
random_search = RandomizedSearchCV(
    rf_clf_model,
    param_distributions=param_dist,
    n_iter=10,  # Reduce the number of sampled parameter combinations
    cv=3,  # Reduce cross-validation folds
    scoring='f1_weighted',  # Use weighted F1 score for imbalanced datasets
    n_jobs=-1,  # Use all available cores
    verbose=1,
    random_state=42
)

# Train the model with the original unscaled training data
random_search.fit(X_train, y_train)

# Print the best hyperparameters found
print(f"Best hyperparameters: {random_search.best_params_}")

# Use the best model found by RandomizedSearchCV to make predictions
best_rf_clf = random_search.best_estimator_
y_pred_best_rf = best_rf_clf.predict(X_test)

# Evaluate the model performance
print("Best Random Forest Classifier with Hyperparameter Tuning")
print("Accuracy:", accuracy_score(y_test, y_pred_best_rf))
print("F1-Score:", f1_score(y_test, y_pred_best_rf, average='weighted'))
print(classification_report(y_test, y_pred_best_rf))

"""The Best Random Forest Classifier with Hyperparameter Tuning achieved an accuracy of 76.6%, with an F1-score of 0.75. For class 0, the model performed well with a precision of 0.82, recall of 0.90, and F1-score of 0.86. However, for class 1, the performance was weaker, with a precision of 0.37, recall of 0.24, and an F1-score of 0.29. The macro average F1-score is 0.57, while the weighted average F1-score is 0.75, indicating that the model is strong for class 0 but still struggles with class 1 despite the hyperparameter tuning.

## 5.8: Tuned XGBoost Model

4. XGBoost


*   max_depth: Maximum depth of each tree.
Why: Balances learning complex patterns without overfitting.
*   earning_rate: Step size for weight updates.
Why: Smaller rates improve generalization but require more boosting rounds.

*   n_estimators: Number of boosting rounds.
Why: More rounds improve performance but risk overfitting.
*   scale_pos_weight: Balance of positive and negative class weights.
Why: Handles class imbalance by increasing the impact of the minority class.

*   subsample: Fraction of samples used for training each tree.
Why: Introduces randomness for better generalization.
*   colsample_bytree: Fraction of features used to build each tree.
Why: Ensures diversity in tree splits to prevent overfitting.
"""

param_dist = {
    'n_estimators': [50, 100, 200],  # Number of boosting rounds
    'max_depth': [3, 5, 7, 10],  # Maximum depth of a tree
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples for training
    'colsample_bytree': [0.6, 0.8, 1.0],  # Fraction of features for training
    'gamma': [0, 1, 5],  # Minimum loss reduction for splits
    'reg_alpha': [0, 0.01, 0.1, 1],  # L1 regularization strength
    'reg_lambda': [1, 1.5, 2],  # L2 regularization strength
    'scale_pos_weight': [1, 2, 10]  # Balances positive/negative class imbalance
}

# Initialize the XGBoost Classifier
xgb_model = XGBClassifier(use_label_encoder=False, random_state=42, eval_metric='logloss')

# Use RandomizedSearchCV to tune hyperparameters
random_search = RandomizedSearchCV(
    xgb_model,
    param_distributions=param_dist,
    n_iter=20,  # Number of parameter settings sampled
    cv=3,  # 3-fold cross-validation
    scoring='f1_weighted',  # Use F1 weighted for imbalanced datasets
    n_jobs=-1,  # Use all available cores
    verbose=1,
    random_state=42
)

# Train the model with the training data
random_search.fit(X_train, y_train)

# Print the best hyperparameters found
print(f"Best hyperparameters: {random_search.best_params_}")

# Use the best model found by RandomizedSearchCV to make predictions
best_xgb = random_search.best_estimator_
y_pred_best_xgb = best_xgb.predict(X_test)

# Evaluate the model performance
print("Best XGBoost Classifier with Hyperparameter Tuning")
print("Accuracy:", accuracy_score(y_test, y_pred_best_xgb))
print("F1-Score:", f1_score(y_test, y_pred_best_xgb, average='weighted'))
print(classification_report(y_test, y_pred_best_xgb))

"""The Best XGBoost Classifier with Hyperparameter Tuning achieved an accuracy of 76.8%, with an F1-score of 0.76. For class 0, the model performed well with a precision of 0.84, recall of 0.87, and F1-score of 0.86. However, for class 1, the performance was weaker, with a precision of 0.41, recall of 0.34, and an F1-score of 0.37. The macro average F1-score is 0.61, and the weighted average F1-score is 0.76, indicating that while the model is strong for class 0, it still faces challenges with class 1 despite the hyperparameter tuning. The hyperparameters used include a subsample of 0.6, scale_pos_weight of 2, reg_lambda of 1, and others that improved the overall performance.

# Part 6: Comparison of Classification Metrics Across Models
"""

from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score # Import recall_score and precision_score
# Define models and predictions
models = [
    ("Logistic Regression", logreg, X_test_scaled, y_pred_logreg),
    ("Decision Tree", dt_clf_model, X_test, y_pred_dt),
    ("Random Forest", rf_model, X_test, y_pred_rf),
    ("XGBoost", xgb_model, X_test, y_pred_xgb)
]

# Initialize lists to hold metrics
model_names = []
accuracies = []
f1_scores = []
recalls = []
precisions = []

# Loop through models to calculate metrics
for name, model, X_test_data, y_pred in models:
    model_names.append(name)
    accuracies.append(accuracy_score(y_test, y_pred))
    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))
    recalls.append(recall_score(y_test, y_pred, average='weighted'))
    precisions.append(precision_score(y_test, y_pred, average='weighted'))

# Create a DataFrame for metrics
data = {
    'Model': model_names,
    'Accuracy': accuracies,
    'F1 Score': f1_scores,
    'Recall': recalls,
    'Precision': precisions
}

model_df = pd.DataFrame(data)

# Initialize lists to hold metrics for tuned models
tuned_model_names = []
tuned_accuracies = []
tuned_f1_scores = []
tuned_recalls = []
tuned_precisions = []

# Add metrics for each tuned model, including XGBoost
tuned_models = [
    ("Tuned Logistic Regression", best_logreg, X_test_scaled, y_pred_best_logreg),
    ("Tuned Decision Tree", best_dt_clf, X_test, y_pred_best_dt),
    ("Tuned Random Forest", best_rf_clf, X_test, y_pred_best_rf),
    ("Tuned XGBoost", best_xgb, X_test, y_pred_best_xgb)
]

# Calculate metrics dynamically
for name, model, X_test_data, y_pred in tuned_models:
    tuned_model_names.append(name)
    tuned_accuracies.append(accuracy_score(y_test, y_pred))
    tuned_f1_scores.append(f1_score(y_test, y_pred, average='weighted'))
    tuned_recalls.append(recall_score(y_test, y_pred, average='weighted'))
    tuned_precisions.append(precision_score(y_test, y_pred, average='weighted'))

# Create a DataFrame for tuned model metrics
tuned_data = {
    'Model': tuned_model_names,
    'Accuracy': tuned_accuracies,
    'F1 Score': tuned_f1_scores,
    'Recall': tuned_recalls,
    'Precision': tuned_precisions
}

tuned_model_df = pd.DataFrame(tuned_data)

# Plotting both original and tuned models side by side
fig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Create a figure with two subplots
sns.set_theme(style="whitegrid")

# Colors for the bars
original_colors = ['#66CDAA', '#3357FF', '#CC5500', '#800000']
tuned_colors = ['#66CDAA', '#3357FF', '#CC5500', '#800000']

# Plot for original models
model_df.set_index('Model').plot(
    kind='bar',
    width=0.8,
    color=original_colors,
    ax=axes[0]
)
axes[0].set_title('Original Models Comparison', fontsize=16)
axes[0].set_xlabel('Models', fontsize=12)
axes[0].set_ylabel('Score', fontsize=12)
axes[0].set_xticks(range(len(model_df)))
axes[0].set_xticklabels(model_df['Model'], rotation=45)
axes[0].set_ylim(0, 1)
axes[0].grid(True, axis='both', linestyle='--', linewidth=0.7, alpha=0.7)
axes[0].legend(loc='upper left', bbox_to_anchor=(1, 1))

# Plot for tuned models
tuned_model_df.set_index('Model').plot(
    kind='bar',
    width=0.8,
    color=tuned_colors,
    ax=axes[1]
)
axes[1].set_title('Tuned Models Comparison', fontsize=16)
axes[1].set_xlabel('Models', fontsize=12)
axes[1].set_ylabel('Score', fontsize=12)
axes[1].set_xticks(range(len(tuned_model_df)))
axes[1].set_xticklabels(tuned_model_df['Model'], rotation=45)
axes[1].set_ylim(0, 1)
axes[1].grid(True, axis='both', linestyle='--', linewidth=0.7, alpha=0.7)
axes[1].legend(loc='upper left', bbox_to_anchor=(1, 1))

# Adjust layout to prevent overlap
plt.tight_layout()

# Show the plots
plt.show()

"""The plots depict the performance comparison of our four machine learning models—Logistic Regression, Decision Tree, Random Forest, and XGBoost—evaluated using Accuracy, F1 Score, Recall, and Precision as metrics. The left plot, titled "Original Models Comparison," shows the baseline performance of the models with default hyperparameters, while the right plot, "Tuned Models Comparison," illustrates their performance after hyperparameter tuning.

After tuning our models. we observed that the regular Logistic Regression model and the regular Random Forest model stayed relatively the same, but we saw greater improvements in the Decision Tree classifier and the XGBoost model post-tuning.

Among the pre-tuned models, the Random Forest Model stands out as the strongest performer in terms of Accuracy, F1 Score, and Recall, making it the best default model. It is slightly behind XGBoost in terms of Precision. After hyperparameter tuning, the most remarkable improvements are observed in the XGBoost model. Its accuracy and F1 Score, and Recall see substantial increases, indicating that tuning has been optimized its and reduced overfitting or underfitting tendencies. Logistic Regression sees moderate gains across all metrics, becoming more balanced in its predictions but still trailing behind Random Forest and XGBoost.

# Part 7: Additional Unsupervised Learning: PCA: Applied to Logistic Regression

In this stage, we applied Principal Component Analysis (PCA) to reduce the dimensionality of the dataset while retaining 90% of the variance. This transformation was applied to both the training and test datasets to create new, lower-dimensional representations of the data. We then visualized the results using 2D and 3D plots. The 2D plot shows the first two principal components, with points colored based on the target class, offering a visual representation of how the data is distributed across these two components. The 3D plot extends this visualization to the first three principal components, providing a more detailed view of the data's structure and relationships in three dimensions. These plots help us better understand the variance and separation in the data, guiding further analysis or model development.
"""

from sklearn.decomposition import PCA

# Apply PCA
pca = PCA(n_components=0.90)  # Retain 90% of the variance
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)


# 2D Plot for the First Two Principal Components
plt.figure(figsize=(8, 6))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=50, alpha=0.7)
plt.title("First Two Principal Components (2D Projection)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label="Target Class")
plt.grid()
plt.show()

# 3D Plot for the First Three Principal Components
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(X_train_pca[:, 0], X_train_pca[:, 1], X_train_pca[:, 2], c=y_train, cmap='viridis', edgecolor='k', s=50, alpha=0.7)
ax.set_title("First Three Principal Components (3D Projection)")
ax.set_xlabel("Principal Component 1")
ax.set_ylabel("Principal Component 2")
ax.set_zlabel("Principal Component 3")
fig.colorbar(scatter, ax=ax, label="Target Class")
plt.show()

# Check the explained variance ratio
print("Explained variance by components:", pca.explained_variance_ratio_)
print("Number of components chosen:", pca.n_components_)

"""
In this stage, we trained a Logistic Regression model on the PCA-transformed data, using the previously reduced features from the first few principal components. After fitting the model to the training data, we evaluated its performance on the test set by predicting the target variable and calculating the accuracy, F1-score, and providing a classification report. The confusion matrix was also computed to visualize the model's performance in terms of true positives, false positives, true negatives, and false negatives. A heatmap was then created for the confusion matrix, offering a clear visual representation of the model's ability to classify the target variable, showing where the model made correct or incorrect predictions."""

# Train Logistic Regression on PCA-transformed data
logreg_pca = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)
logreg_pca.fit(X_train_pca, y_train)

# Predict and evaluate Logistic Regression with PCA
y_pred_pca = logreg_pca.predict(X_test_pca)

print("Logistic Regression with PCA")
print("Accuracy:", accuracy_score(y_test, y_pred_pca))
print("F1-Score:", f1_score(y_test, y_pred_pca))
print(classification_report(y_test, y_pred_pca))

# Confusion Matrix
cm_pca = confusion_matrix(y_test, y_pred_pca)

# Create a heatmap for Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm_pca, annot=True, fmt='d', cmap='Blues', xticklabels=logreg_pca.classes_, yticklabels=logreg_pca.classes_)
plt.title('Confusion Matrix: Logistic Regression with PCA')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""This confusion matrix, paired with classification metrics, evaluates the performance of a Logistic Regression model applied with Principal Component Analysis (PCA). The accuracy is 66.63%, indicating the percentage of correct predictions overall. The F1-score is 0.40, highlighting a moderate balance between precision (0.31) and recall (0.56) for class 1. The model performs well for class 0, with a high precision (0.86) and moderate recall (0.69), but struggles with class 1, which suffers from poor precision and moderate recall. This suggests that while the model is generally effective for identifying the majority class (0), it has difficulty accurately classifying the minority class (1), leading to potential imbalances in performance.

# Part 8: Results Summary

In this project, we conducted a comprehensive performance analysis of four machine learning models—Logistic Regression, Decision Tree, Random Forest, and XGBoost—using Accuracy, F1 Score, Precision, and Recall as evaluation metrics. The models were assessed both with default hyperparameters and after hyperparameter tuning to determine their strengths and limitations.

Key Findings:
- Pre-tuned Models: The Random Forest model emerged as the best performer in the default configuration, excelling in Accuracy, F1 Score, and Recall. However, its Precision for the minority class (class 1) was slightly lower. XGBoost showed promise with its precision-oriented performance but fell short in overall metrics compared to Random Forest.
- Post-tuned Models: Hyperparameter tuning yielded notable improvements in the XGBoost model, which achieved the highest overall performance post-tuning, with an accuracy of 76.8% and a weighted F1-score of 0.76. While it performed well on the majority class (class 0),the precision was 0.41 and F1-score was 0.76. The Random Forest model, with an accuracy of 76.6% and a weighted F1-score of 0.75, demonstrated balanced improvements post-tuning but faced challenges similar to XGBoost in handling class 1. The Decision Tree model showed moderate gains, achieving an accuracy of 75.9% and a weighted F1-score of 0.75, but its class 1 performance remained weak. The Logistic Regression model had the lowest overall performance, with limited gains from tuning. It achieved an accuracy of 66.5% and a weighted F1-score of 0.69, reflecting its challenges in handling class 1 predictions effectively.

The XGBoost model stands out as the most robust classifier after hyperparameter tuning, delivering strong performance across most metrics and substantial gains in accuracy and recall. However, all models faced difficulties with class 1, emphasizing the need for further optimization or advanced techniques such as class balancing or ensemble strategies to address the class imbalance. The project highlights the importance of hyperparameter tuning and its impact on enhancing model performance, particularly for complex algorithms like XGBoost.

# Part 8: Challenges/Obstacles Faced


The dataset used in our analysis exhibited a significant class imbalance, with "No-shows" being much fewer than "Shows." To address this, we stratified our data split based on the no-show variable to ensure proportional representation in training and testing. Additionally, we incorporated class weights during model training to mitigate the imbalance's impact. Despite these efforts, the limited dataset size constrained the model's overall performance. Furthermore, the dataset lacked potentially critical features, such as socioeconomic data, which could have enhanced our ability to predict no-shows accurately. Another limitation is the dataset’s geographic scope, as it was restricted to Vitoria. Patients with longer intervals between appointment booking and attendance might have sought care in neighboring cities or at healthcare institutions outside Vitoria. Finally, it is important to note that healthcare in Brazil is free, which may influence appointment adherence, as individuals may not perceive as strong an obligation to attend their appointments compared to settings where healthcare is a direct financial investment.

# Part 8: Potential Next Steps

To enhance the predictive power and generalizability of our models, future work should focus on gathering data from multiple hospitals or regions, incorporating more diverse patient demographics and socioeconomic factors. Including socioeconomic features in the dataset could provide valuable insights into the underlying factors contributing to medical appointment no-shows. Additionally, exploring alternative modeling approaches, such as unsupervised learning techniques, could uncover hidden patterns or clusters within the data. To make our findings actionable, we recommend developing an interactive dashboard for hospital staff. This dashboard could help prioritize patients at higher risk of no-shows, enabling targeted interventions to improve attendance rates and optimize resource allocation.

# Part 10: Conclusion

In this project, we implemented a comprehensive data analysis pipeline, training binary classification machine learning models to predict medical appointment no-shows. Our efforts included optimizing model performance through hyperparameter tuning and leveraging advanced techniques such as class balancing and ensemble strategies. Despite these measures, accurately predicting the minority class—the no-shows—remained challenging, highlighting the limitations imposed by class imbalance and the need for further refinement of our approach. Importantly, we prioritized recall as a key metric, given its critical role in identifying potential no-shows for targeted interventions. This project underscores the importance of a well-structured data analysis pipeline, the value of hyperparameter tuning for optimizing complex models, and the potential of predictive analytics to enhance healthcare delivery. Our findings suggest that predictive models, when further improved, could enable healthcare providers to better anticipate no-shows, streamline resource allocation, and reduce patient wait times, ultimately contributing to more efficient and effective patient care.
"""